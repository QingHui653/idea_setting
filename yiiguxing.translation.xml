<application>
  <component name="AppStorage">
    <histories>
      <item value="Attempting to write a row[2] in the range [0,6] that is already written to disk." />
      <item value="the number of rows that are kept in memory until flushed out, see above." />
      <item value="row Access Window Size" />
      <item value="Stamped" />
      <item value="Stamped Lock" />
      <item value="Stack" />
      <item value="sto sn sku" />
      <item value="SERIAL NUMBER" />
      <item value="ensure" />
      <item value="The segments, each of which is a specialized hash table." />
      <item value="Shift value for indexing within segments." />
      <item value="ask value for indexing into segments. The upper bits of a * key's hash code are used to choose the segment." />
      <item value="Mask" />
      <item value="segment Mask" />
      <item value="ordered write of segments[0]" />
      <item value="sshift" />
      <item value="segment Shift" />
      <item value="Find power-of-two sizes best matching arguments" />
      <item value="MAX SEGMENTS" />
      <item value="concurrency Level" />
      <item value="Segment" />
      <item value="slightly conservative" />
      <item value="RETRIES BEFORE LOCK" />
      <item value="This class implements a tree-like two-dimensionally linked skip * list in which the index levels are represented in separate * nodes from the base nodes holding data. There are two reasons * for taking this approach instead of the usual array-based * structure: 1) Array based implementations seem to encounter * more complexity and overhead 2) We can use cheaper algorithms * for the heavily-traversed index lists than can be used for the * base lists. Here's a picture of some of the basics for a * possible list with 2 levels of index:" />
      <item value="A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. A CountDownLatch is initialized with a given count. The await methods block until the current count reaches zero due to invocations of the countDown method, after which all waiting threads are released and any subsequent invocations of await return immediately. This is a one-shot phenomenon -- the count cannot be reset. If you need a version that resets the count, consider using a CyclicBarrier. A CountDownLatch is a versatile synchronization tool and can be used for a number of purposes. A CountDownLatch initialized with a count of one serves as a simple on/off latch, or gate: all threads invoking await wait at the gate until it is opened by a thread invoking countDown. A CountDownLatch initialized to N can be used to make one thread wait until N threads have completed some action, or some action has been completed N times. A useful property of a CountDownLatch is that it doesn't require that threads calling countDown wait for the count to reach zero before proceeding, it simply prevents any thread from proceeding past an await until all threads could pass." />
      <item value="Reentrant" />
      <item value="Reentrant Lock" />
      <item value="The lock protecting all mutators" />
      <item value="Acquires a permit from this semaphore, blocking until one is available, or the thread is interrupted. Acquires a permit, if one is available and returns immediately, reducing the number of available permits by one. If no permit is available then the current thread becomes disabled for thread scheduling purposes and lies dormant until one of two things happens: Some other thread invokes the release method for this semaphore and the current thread is next to be assigned a permit; or Some other thread interrupts the current thread. If the current thread: has its interrupted status set on entry to this method; or is interrupted while waiting for a permit, then InterruptedException is thrown and the current thread's interrupted status is cleared." />
      <item value="Acquires a permit from this semaphore, blocking until one is available, or the thread is interrupted. Acquires a permit, if one is available and returns immediately, reducing the number of available permits by one. If no permit is available then the current thread becomes disabled for thread scheduling purposes and lies dormant until one of two things happens: Some other thread invokes the release method for this semaphore and the current thread is next to be assigned a permit; or Some other thread interrupts the current thread. If the current thread: has its interrupted status set on entry to this method; or is interrupted while waiting for a permit," />
      <item value="release" />
      <item value="acquire" />
      <item value="Creates a Semaphore with the given number of permits and nonfair fairness setting." />
      <item value="Wakes up a single thread that is waiting on this object's monitor. If any threads are waiting on this object, one of them is chosen to be awakened. The choice is arbitrary and occurs at the discretion of the implementation. A thread waits on an object's monitor by calling one of the wait methods. The awakened thread will not be able to proceed until the current thread relinquishes the lock on this object. The awakened thread will compete in the usual manner with any other threads that might be actively competing to synchronize on this object; for example, the awakened thread enjoys no reliable privilege or disadvantage in being the next thread to lock this object. This method should only be called by a thread that is the owner of this object's monitor. A thread becomes the owner of the object's monitor in one of three ways:" />
      <item value="ReentrantReadWriteLocks can be used to improve concurrency in some * uses of some kinds of Collections. This is typically worthwhile * only when the collections are expected to be large, accessed by * more reader threads than writer threads, and entail operations with * overhead that outweighs synchronization overhead. For example, here * is a class using a TreeMap that is expected to be large and * concurrently accessed." />
      <item value="Basic thread blocking primitives for creating locks and other synchronization classes. This class associates, with each thread that uses it, a permit (in the sense of the Semaphore class). A call to park will return immediately if the permit is available, consuming it in the process; otherwise it may block. A call to unpark makes the permit available, if it was not already available. (Unlike with Semaphores though, permits do not accumulate. There is at most one.) Methods park and unpark provide efficient means of blocking and unblocking threads that do not encounter the problems that cause the deprecated methods Thread.suspend and Thread.resume to be unusable for such purposes: Races between one thread invoking park and another thread trying to unpark it will preserve liveness, due to the permit. Additionally, park will return if the caller's thread was interrupted, and timeout versions are supported. The park method may also return at any other time, for &quot;no reason&quot;, so in general must be invoked within a loop that rechecks conditions upon return. In this sense park serves as an optimization of a &quot;busy wait&quot; that does not waste as much time spinning, but must be paired with an unpark to be effective. The three forms of park each also support a blocker object parameter. This object is recorded while the thread is blocked to permit monitoring and diagnostic tools to identify the reasons that threads are blocked. (Such tools may access blockers using method getBlocker.) The use of these forms rather than the original forms without this parameter is strongly encouraged. The normal argument to supply as a blocker within a lock implementation is this. These methods are designed to be used as tools for creating higher-level synchronization utilities, and are not in themselves useful for most concurrency control applications. The park method is designed for use only in constructions of the form: while (!canProceed()) { ... LockSupport.park(this); } where neither canProceed nor any other actions prior to the call to park entail locking or blocking. Because only one permit is associated with each thread, any intermediary uses of park could interfere with its intended effects." />
      <item value="A version of AbstractQueuedSynchronizer in which synchronization state is maintained as a long. This class has exactly the same structure, properties, and methods as AbstractQueuedSynchronizer with the exception that all state-related parameters and results are defined as long rather than int. This class may be useful when creating synchronizers such as multilevel locks and barriers that require 64 bits of state." />
      <item value="A synchronizer that may be exclusively owned by a thread. This class provides a basis for creating locks and related synchronizers that may entail a notion of ownership. The AbstractOwnableSynchronizer class itself does not manage or use this information. However, subclasses and tools may use appropriately maintained values to help control and monitor access and provide diagnostics." />
      <item value="Condition factors out the Object monitor methods (wait, notify and notifyAll) into distinct objects to give the effect of having multiple wait-sets per object, by combining them with the use of arbitrary Lock implementations. Where a Lock replaces the use of synchronized methods and statements, a Condition replaces the use of the Object monitor methods. Conditions (also known as condition queues or condition variables) provide a means for one thread to suspend execution (to &quot;wait&quot;) until notified by another thread that some state condition may now be true. Because access to this shared state information occurs in different threads, it must be protected, so a lock of some form is associated with the condition. The key property that waiting for a condition provides is that it atomically releases the associated lock and suspends the current thread, just like Object.wait. A Condition instance is intrinsically bound to a lock. To obtain a Condition instance for a particular Lock instance use its newCondition() method. As an example, suppose we have a bounded buffer which supports put and take methods. If a take is attempted on an empty buffer, then the thread will block until an item becomes available; if a put is attempted on a full buffer, then the thread will block until a space becomes available. We would like to keep waiting put threads and take threads in separate wait-sets so that we can use the optimization of only notifying a single thread at a time when items or spaces become available in the buffer. This can be achieved using two Condition instances." />
      <item value="A reflection-based utility that enables atomic updates to designated volatile int fields of designated classes. This class is designed for use in atomic data structures in which several fields of the same node are independently subject to atomic updates. Note that the guarantees of the compareAndSet method in this class are weaker than in other atomic classes. Because this class cannot ensure that all uses of the field are appropriate for purposes of atomic access, it can guarantee atomicity only with respect to other invocations of compareAndSet and set on the same updater." />
      <item value="A boolean value that may be updated atomically. See the java.util.concurrent.atomic package specification for description of the properties of atomic variables. An AtomicBoolean is used in applications such as atomically updated flags, and cannot be used as a replacement for a Boolean." />
      <item value="Eventually sets to the given value." />
      <item value="Atomically sets to the given value and returns the previous value. *" />
      <item value="Atomically sets to the given value and returns the previous value." />
      <item value="Unconditionally sets to the given value." />
      <item value="Returns the current value." />
      <item value="setup to use Unsafe.compareAndSwapInt for updates" />
      <item value="A scalable concurrent ConcurrentNavigableMap implementation. The map is sorted according to the natural ordering of its keys, or by a Comparator provided at map creation time, depending on which constructor is used. This class implements a concurrent variant of SkipLists providing expected average log(n) time cost for the containsKey, get, put and remove operations and their variants. Insertion, removal, update, and access operations safely execute concurrently by multiple threads. Iterators are weakly consistent, returning elements reflecting the state of the map at some point at or since the creation of the iterator. They do not throw ConcurrentModificationException, and may proceed concurrently with other operations. Ascending key ordered views and their iterators are faster than descending ones. All Map.Entry pairs returned by methods in this class and its views represent snapshots of mappings at the time they were produced. They do not support the Entry.setValue method. (Note however that it is possible to change mappings in the associated map using put, putIfAbsent, or replace, depending on exactly which effect you need.) Beware that, unlike in most collections, the size method is not a constant-time operation. Because of the asynchronous nature of these maps, determining the current number of elements requires a traversal of the elements, and so may report inaccurate results if this collection is modified during traversal. Additionally, the bulk operations putAll, equals, toArray, containsValue, and clear are not guaranteed to be performed atomically. For example, an iterator operating concurrently with a putAll operation might view only some of the added elements. This class and its views and iterators implement all of the optional methods of the Map and Iterator interfaces. Like most other concurrent collections, this class does not permit the use of null keys or values because some null return values cannot be reliably distinguished from the absence of elements." />
      <item value="Implementation Overview * * This class provides the central bookkeeping and control for a * set of worker threads: Submissions from non-FJ threads enter * into a submission queue. Workers take these tasks and typically * split them into subtasks that may be stolen by other workers. * Preference rules give first priority to processing tasks from * their own queues (LIFO or FIFO, depending on mode), then to * randomized FIFO steals of tasks in other worker queues, and * lastly to new submissions." />
      <item value="/* * Implementation Overview * * This class provides the central bookkeeping and control for a * set of worker threads: Submissions from non-FJ threads enter * into a submission queue. Workers take these tasks and typically * split them into subtasks that may be stolen by other workers. * Preference rules give first priority to processing tasks from * their own queues (LIFO or FIFO, depending on mode), then to * randomized FIFO steals of tasks in other worker queues, and * lastly to new submissions. * * The main throughput advantages of work-stealing stem from * decentralized control -- workers mostly take tasks from * themselves or each other. We cannot negate this in the * implementation of other management responsibilities. The main * tactic for avoiding bottlenecks is packing nearly all * essentially atomic control state into a single 64bit volatile * variable (&quot;ctl&quot;). This variable is read on the order of 10-100 * times as often as it is modified (always via CAS). (There is * some additional control state, for example variable &quot;shutdown&quot; * for which we can cope with uncoordinated updates.) This * streamlines synchronization and control at the expense of messy * constructions needed to repack status bits upon updates. * Updates tend not to contend with each other except during * bursts while submitted tasks begin or end. In some cases when * they do contend, threads can instead do something else * (usually, scan for tasks) until contention subsides. * * To enable packing, we restrict maximum parallelism to (1&lt;&lt;15)-1 * (which is far in excess of normal operating range) to allow * ids, counts, and their negations (used for thresholding) to fit * into 16bit fields. * * Recording Workers. Workers are recorded in the &quot;workers&quot; array * that is created upon pool construction and expanded if (rarely) * necessary. This is an array as opposed to some other data * structure to support index-based random steals by workers. * Updates to the array recording new workers and unrecording * terminated ones are protected from each other by a seqLock * (scanGuard) but the array is otherwise concurrently readable, * and accessed directly by workers. To simplify index-based * operations, the array size is always a power of two, and all * readers must tolerate null slots. To avoid flailing during * start-up, the array is presized to hold twice #parallelism * workers (which is unlikely to need further resizing during * execution). But to avoid dealing with so many null slots, * variable scanGuard includes a mask for the nearest power of two * that contains all current workers. All worker thread creation * is on-demand, triggered by task submissions, replacement of * terminated workers, and/or compensation for blocked * workers. However, all other support code is set up to work with * other policies. To ensure that we do not hold on to worker * references that would prevent GC, ALL accesses to workers are * via indices into the workers array (which is one source of some * of the messy code constructions here). In essence, the workers * array serves as a weak reference mechanism. Thus for example * the wait queue field of ctl stores worker indices, not worker * references. Access to the workers in associated methods (for * example signalWork) must both index-check and null-check the * IDs. All such accesses ignore bad IDs by returning out early * from what they are doing, since this can only be associated * with termination, in which case it is OK to give up. * * All uses of the workers array, as well as queue arrays, check * that the array is non-null (even if previously non-null). This * allows nulling during termination, which is currently not * necessary, but remains an option for resource-revocation-based * shutdown schemes. * * Wait Queuing. Unlike HPC work-stealing frameworks, we cannot * let workers spin indefinitely scanning for tasks when none can * be found immediately, and we cannot start/resume workers unless * there appear to be tasks available. On the other hand, we must * quickly prod them into action when new tasks are submitted or * generated. We park/unpark workers after placing in an event * wait queue when they cannot find work. This &quot;queue&quot; is actually * a simple Treiber stack, headed by the &quot;id&quot; field of ctl, plus a * 15bit counter value to both wake up waiters (by advancing their * count) and avoid ABA effects. Successors are held in worker * field &quot;nextWait&quot;. Queuing deals with several intrinsic races, * mainly that a task-producing thread can miss seeing (and * signalling) another thread that gave up looking for work but * has not yet entered the wait queue. We solve this by requiring * a full sweep of all workers both before (in scan()) and after * (in tryAwaitWork()) a newly waiting worker is added to the wait * queue. During a rescan, the worker might release some other * queued worker rather than itself, which has the same net * effect. Because enqueued workers may actually be rescanning * rather than waiting, we set and clear the &quot;parked&quot; field of * ForkJoinWorkerThread to reduce unnecessary calls to unpark. * (Use of the parked field requires a secondary recheck to avoid * missed signals.) * * Signalling. We create or wake up workers only when there * appears to be at least one task they might be able to find and * execute. When a submission is added or another worker adds a * task to a queue that previously had two or fewer tasks, they * signal waiting workers (or trigger creation of new ones if * fewer than the given parallelism level -- see signalWork). * These primary signals are buttressed by signals during rescans * as well as those performed when a worker steals a task and * notices that there are more tasks too; together these cover the * signals needed in cases when more than two tasks are pushed * but untaken. * * Trimming workers. To release resources after periods of lack of * use, a worker starting to wait when the pool is quiescent will * time out and terminate if the pool has remained quiescent for * SHRINK_RATE nanosecs. This will slowly propagate, eventually * terminating all workers after long periods of non-use. * * Submissions. External submissions are maintained in an * array-based queue that is structured identically to * ForkJoinWorkerThread queues except for the use of * submissionLock in method addSubmission. Unlike the case for * worker queues, multiple external threads can add new * submissions, so adding requires a lock. * * Compensation. Beyond work-stealing support and lifecycle * control, the main responsibility of this framework is to take * actions when one worker is waiting to join a task stolen (or * always held by) another. Because we are multiplexing many * tasks on to a pool of workers, we can't just let them block (as * in Thread.join). We also cannot just reassign the joiner's * run-time stack with another and replace it later, which would * be a form of &quot;continuation&quot;, that even if possible is not * necessarily a good idea since we sometimes need both an * unblocked task and its continuation to progress. Instead we * combine two tactics: * * Helping: Arranging for the joiner to execute some task that it * would be running if the steal had not occurred. Method * ForkJoinWorkerThread.joinTask tracks joining-&gt;stealing * links to try to find such a task. * * Compensating: Unless there are already enough live threads, * method tryPreBlock() may create or re-activate a spare * thread to compensate for blocked joiners until they * unblock. * * The ManagedBlocker extension API can't use helping so relies * only on compensation in method awaitBlocker. * * It is impossible to keep exactly the target parallelism number * of threads running at any given time. Determining the * existence of conservatively safe helping targets, the * availability of already-created spares, and the apparent need * to create new spares are all racy and require heuristic * guidance, so we rely on multiple retries of each. Currently, * in keeping with on-demand signalling policy, we compensate only * if blocking would leave less than one active (non-waiting, * non-blocked) worker. Additionally, to avoid some false alarms * due to GC, lagging counters, system activity, etc, compensated * blocking for joins is only attempted after rechecks stabilize * (retries are interspersed with Thread.yield, for good * citizenship). The variable blockedCount, incremented before * blocking and decremented after, is sometimes needed to * distinguish cases of waiting for work vs blocking on joins or * other managed sync. Both cases are equivalent for most pool * control, so we can update non-atomically. (Additionally, * contention on blockedCount alleviates some contention on ctl). * * Shutdown and Termination. A call to shutdownNow atomically sets * the ctl stop bit and then (non-atomically) sets each workers * &quot;terminate&quot; status, cancels all unprocessed tasks, and wakes up * all waiting workers. Detecting whether termination should * commence after a non-abrupt shutdown() call requires more work * and bookkeeping. We need consensus about quiesence (i.e., that * there is no more work) which is reflected in active counts so * long as there are no current blockers, as well as possible * re-evaluations during independent changes in blocking or * quiescing workers. * * Style notes: There is a lot of representation-level coupling * among classes ForkJoinPool, ForkJoinWorkerThread, and * ForkJoinTask. Most fields of ForkJoinWorkerThread maintain * data structures managed by ForkJoinPool, so are directly * accessed. Conversely we allow access to &quot;workers&quot; array by * workers, and direct access to ForkJoinTask.status by both * ForkJoinPool and ForkJoinWorkerThread. There is little point * trying to reduce this, since any associated future changes in * representations will need to be accompanied by algorithmic * changes anyway. All together, these low-level implementation * choices produce as much as a factor of 4 performance * improvement compared to naive implementations, and enable the * processing of billions of tasks per second, at the expense of * some ugliness. * * Methods signalWork() and scan() are the main bottlenecks so are * especially heavily micro-optimized/mangled. There are lots of * inline assignments (of form &quot;while ((local = field) != 0)&quot;) * which are usually the simplest way to ensure the required read * orderings (which are sometimes critical). This leads to a * &quot;C&quot;-like style of listing declarations of these locals at the * heads of methods or blocks. There are several occurrences of * the unusual &quot;do {} while (!cas...)&quot; which is the simplest way * to force an update of a CAS'ed variable. There are also other * coding oddities that help some methods perform reasonably even * when interpreted (not compiled). * * The order of declarations in this file is: (1) declarations of * statics (2) fields (along with constants used when unpacking * some of them), listed in an order that tends to reduce * contention among them a bit under most JVMs. (3) internal * control methods (4) callbacks and other support for * ForkJoinTask and ForkJoinWorkerThread classes, (5) exported * methods (plus a few little helpers). (6) static block * initializing all statics in a minimally dependent order. */" />
    </histories>
  </component>
  <component name="Settings">
    <option name="ignoreRegExp" value="" />
  </component>
</application>